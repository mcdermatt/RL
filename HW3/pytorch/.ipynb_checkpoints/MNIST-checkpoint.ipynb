{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo NN using builtin datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download datasets locally\n",
    "train = datasets.MNIST(\"\", train = True, download = True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train = False, download = True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batches - don't want to give the algo all the data at once or it will not work as well on new data\n",
    "#   usually best batch size is between 8 and 64\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0, 7, 2, 0, 2, 9, 4, 5, 6, 3])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)\n",
    "\n",
    "#torch.Size([1, 28, 28])\n",
    "#   size of image is actually an extra dimension\n",
    "#   extra 1 dimension is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2594467f488>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing is SUPER important-\n",
    "if 60% of our training data is a \"3\" then a set of weights that always guesses 3 will reach a local max and training will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "        \n",
    "print(counter_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): #create class Net and inherit from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__() #need to run this because the init func of nn.Module is not run from inherit\n",
    "       \n",
    "        #Linear is a simple flat fuly connected\n",
    "        self.fc1 = nn.Linear(784, 64) #when images are flattened they are 28*28 = 784 long\n",
    "        self.fc2 = nn.Linear(64, 64)  #arbitrarily choosing 64 nodes for hidden layers\n",
    "        self.fc3 = nn.Linear(64, 64) \n",
    "        self.fc4 = nn.Linear(64, 10)  #output layer is size 10 for digits 0-9\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #F.relu is rectified linear activation func\n",
    "        #   activation func is sigmoid- keeps output from exploding\n",
    "        #   attempt to model whether neuron is or is not firing\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #for output we only want one neuron to be fully fired\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim = 1) #since I am working with flat linear funcs this dim will always = 1\n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4356, -2.2765, -2.2231, -2.3814, -2.2195, -2.4148, -2.4248, -2.2997,\n",
      "         -2.1344, -2.2636]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand((28,28)) #fake input image\n",
    "X = X.view(-1,28*28)    #-1 specifies that input will be of unknown shape\n",
    "output = net(X)\n",
    "print(output) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1359, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0062, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#loss is a measure of how wrong is our model\n",
    "#   loss should trend downward over time\n",
    "# Optimizer adjusts weights bit by bit over time (according to learning rate) to lower loss\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001) #net.parmeters controls what stuff in net() is adjusted (default is everything)\n",
    "\n",
    "#decaying learning rate -> lr decreases over time to prevent overshooting\n",
    "\n",
    "EPOCHS = 3 #full passes through dataset\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        #data is a batch of featuresets and labels\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output, y) #usually use mean^2 error if data is onehot vector\n",
    "        \n",
    "        #magic backpropogation algorithm\n",
    "        loss.backward()\n",
    "        \n",
    "        #adjusts weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    print (loss)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.976\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    #net.train() and net.eval() are depreciated ways of changing the mode\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        \n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "                \n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
